{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.24 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from monai) (1.26.2)\n",
      "Requirement already satisfied: torch>=1.9 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from monai) (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch>=1.9->monai) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch>=1.9->monai) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch>=1.9->monai) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch>=1.9->monai) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch>=1.9->monai) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch>=1.9->monai) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
      "Requirement already satisfied: torch in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: torchvision in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: filelock in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: numpy in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/costanzasiniscalchi/miniconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install monai\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9539210', '6008569', '4276824', '9909448', '2887681', '5030375', '8478383', '1984879', '8052813', '4773593', '3343577', '7125565', '5158901', '4210363', '3301724', '2963960', '5187625', '7755697', '4317780', '1004359', '2823276', '7237992', '3475739', '1369125', '6967785', '1016072', '3048898', '3191214', '3730353', '3162878', '3692881', '9827494', '4052945', '5160587', '4040157', '5692079', '3165520', '8120729', '7760229', '2448082', '4943065', '2924615', '7863867', '9380004', '1635604', '4136011', '7550757', '8686311', '2599481', '3705605', '4210489', '7672530', '5730499', '4532706', '6433158']\n",
      "55\n",
      "Train IDs: ['1004359', '3692881', '7125565', '5158901', '4210363', '4136011', '4276824', '3301724', '3343577', '4052945', '2599481', '3191214', '7550757', '5730499', '1016072', '3705605', '1369125', '2963960', '3048898', '7672530', '4040157', '2887681', '9909448', '5187625', '1984879', '1635604', '3162878', '8120729', '3475739', '4943065', '9539210', '3730353', '6433158', '8686311', '3165520', '5030375', '4773593', '2823276']\n",
      "Test IDs: ['7760229', '8478383', '9380004', '4532706', '7755697', '2924615', '8052813', '7863867']\n",
      "Validation IDs: ['6008569', '2448082', '4210489', '5160587', '6967785', '5692079', '7237992', '4317780', '9827494']\n"
     ]
    }
   ],
   "source": [
    "## Train Test Val Split\n",
    "\n",
    "# 80 - 10 - 10 split\n",
    "\n",
    "import random\n",
    "import os\n",
    "## train test split\n",
    "np_dir = '/Users/costanzasiniscalchi/Documents/MS/ADL/Project/ADL_Project/data/numpy_conversions_3_scans'\n",
    "ids = os.listdir(np_dir)\n",
    "print(ids)\n",
    "print(len(ids))\n",
    "def split_data(ids, train_size=0.7, test_size=0.15, validation_size=0.15):\n",
    "    # Shuffle the list of IDs\n",
    "    random.shuffle(ids)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    total_size = len(ids)\n",
    "    train_end = int(train_size * total_size)\n",
    "    test_end = train_end + int(test_size * total_size)\n",
    "    \n",
    "    # Split the data\n",
    "    train_ids = ids[:train_end]\n",
    "    test_ids = ids[train_end:test_end]\n",
    "    validation_ids = ids[test_end:]\n",
    "    \n",
    "    return train_ids, test_ids, validation_ids\n",
    "\n",
    "# Example usage:\n",
    "train, test, validation = split_data(ids)\n",
    "\n",
    "print(\"Train IDs:\", train)\n",
    "print(\"Test IDs:\", test)\n",
    "print(\"Validation IDs:\", validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.transforms import Compose, ScaleIntensity, ToTensor\n",
    "import torch\n",
    "\n",
    "class MRIDataLoader:\n",
    "    def __init__(self, numpy_dir, id_list, random_order = True, transform=None):\n",
    "        self.numpy_dir = numpy_dir\n",
    "        self.transform = transform\n",
    "        self.patient_ids = id_list\n",
    "        self.random_order = random_order\n",
    "        self.data, self.labels = self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "        all_data = []\n",
    "        labels = []\n",
    "        for patient_id in self.patient_ids:\n",
    "            # List the scan dates for each patient\n",
    "            scan_dates = sorted(os.listdir(os.path.join(self.numpy_dir, patient_id)))\n",
    "            patient_data = []\n",
    "            patient_label = []\n",
    "            for scan_date in scan_dates:\n",
    "                # Get the corresponding numpy file path\n",
    "                numpy_file = os.path.join(self.numpy_dir, patient_id, scan_date, f\"preventad_{patient_id}_{scan_date}_t1w_001_t1w-defaced_001.npy\")\n",
    "                \n",
    "                # Check if the numpy file exists\n",
    "                if os.path.exists(numpy_file):\n",
    "                    patient_data.append(numpy_file)  # Append MRI file path\n",
    "                    # Assign the label based on the scan date\n",
    "                    if scan_date == 'PREBL00':\n",
    "                        patient_label.append(0)\n",
    "                    elif scan_date == 'PREFU12':\n",
    "                        patient_label.append(1)\n",
    "                    else:\n",
    "                        patient_label.append(2)\n",
    "            all_data.append(patient_data)\n",
    "            labels.append(patient_label)\n",
    "\n",
    "        return all_data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        numpy_files = self.data[idx]\n",
    "        \n",
    "        label = self.labels[idx]  # Get the corresponding label\n",
    "        \n",
    "        if self.random_order:\n",
    "            # Shuffle the files and labels together\n",
    "            combined = list(zip(numpy_files, label))\n",
    "            random.shuffle(combined)\n",
    "            numpy_files, label = zip(*combined)  # Unzip back to separate lists\n",
    "\n",
    "        numpy_data = []\n",
    "        # Load corresponding numpy file\n",
    "        for file in numpy_files:\n",
    "            # Apply transformations if provided\n",
    "            data = np.load(file)  # Load the scan as a numpy array\n",
    "            if self.transform:\n",
    "                data = self.transform(data)  # Apply transformations\n",
    "            numpy_data.append(data)\n",
    "        \n",
    "        # Stack the data for the sequence of scans\n",
    "        numpy_data = np.stack(numpy_data, axis=0)  # Shape will be (num_scans, channels, D, H, W)\n",
    "\n",
    "        # Return the data as a dictionary\n",
    "        return {\"labels\": torch.tensor(label, dtype=torch.long), \"numpy\": torch.tensor(numpy_data, dtype=torch.float32)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Sample data shape: torch.Size([4, 3, 176, 256, 240])\n",
      "Sample labels: tensor([[2, 1, 0],\n",
      "        [0, 1, 2],\n",
      "        [1, 2, 0],\n",
      "        [1, 2, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Define a transformation pipeline for MRI data\n",
    "transform = Compose([ScaleIntensity(), ToTensor()])\n",
    "\n",
    "# Initialize the dataset loader\n",
    "train_mri_loader = MRIDataLoader(numpy_dir=np_dir, id_list = train, transform=transform)\n",
    "test_mri_loader = MRIDataLoader(numpy_dir=np_dir, id_list = test, transform=transform)\n",
    "val_mri_loader = MRIDataLoader(numpy_dir=np_dir, id_list = validation, transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_mri_loader, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_mri_loader, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_mri_loader, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "print(len(train_mri_loader))\n",
    "sample = next(iter(train_loader)) # check if iterating works\n",
    "print(\"Sample data shape:\", sample['numpy'].shape)  # Check the shape of the data\n",
    "\n",
    "# Sample label\n",
    "print(\"Sample labels:\", sample['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(1, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (1-2): 2 x AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (3): AEKLDownsample(\n",
       "        (pad): AsymmetricPad()\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
       "        )\n",
       "      )\n",
       "      (4): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (6): AEKLDownsample(\n",
       "        (pad): AsymmetricPad()\n",
       "        (conv): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
       "        )\n",
       "      )\n",
       "      (7): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (9): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "      (10): Convolution(\n",
       "        (conv): Conv3d(256, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Convolution(\n",
       "        (conv): Conv3d(8, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "      (1-2): 2 x AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (3): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='nearest')\n",
       "        (postconv): Convolution(\n",
       "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (4): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (5): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (6): UpSample(\n",
       "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='nearest')\n",
       "        (postconv): Convolution(\n",
       "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (7): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Convolution(\n",
       "          (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "        )\n",
       "      )\n",
       "      (8): AEKLResBlock(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv1): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "        (conv2): Convolution(\n",
       "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        )\n",
       "        (nin_shortcut): Identity()\n",
       "      )\n",
       "      (9): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "      (10): Convolution(\n",
       "        (conv): Conv3d(64, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quant_conv_mu): Convolution(\n",
       "    (conv): Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (quant_conv_log_sigma): Convolution(\n",
       "    (conv): Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       "  (post_quant_conv): Convolution(\n",
       "    (conv): Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from monai.networks.nets import AutoencoderKL\n",
    "\n",
    "# Define the autoencoder model (make sure it matches the saved model architecture)\n",
    "autoencoder_model = AutoencoderKL(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    latent_channels=8,\n",
    "    channels=[64, 128, 256],\n",
    "    num_res_blocks=2,\n",
    "    norm_num_groups=32,\n",
    "    norm_eps=1e-06,\n",
    "    attention_levels=[False, False, False],\n",
    "    with_encoder_nonlocal_attn=False,\n",
    "    with_decoder_nonlocal_attn=False,\n",
    "    include_fc=False\n",
    ")\n",
    "\n",
    "# Load the pretrained weights\n",
    "autoencoder_path = '/Users/costanzasiniscalchi/Documents/MS/ADL/Project/ADL_Project/monai_brats_mri_generative_diffusion_1.1.2/models/model_autoencoder.pt'\n",
    "state_dict = torch.load(autoencoder_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Load the state dict with strict=False to ignore mismatched keys\n",
    "autoencoder_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "autoencoder_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train_model(train_loader, model, encoder, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"numpy\"].to(device)  # Shape: (batch_size, num_scans, channels, D, H, W)\n",
    "        labels = batch[\"labels\"].to(device)  # Shape: (batch_size, num_scans)\n",
    "\n",
    "        batch_loss = 0\n",
    "        for i in range(inputs.size(1)):  # Loop over each scan in the batch\n",
    "            # Get the latent representation for the current scan\n",
    "            scan_input = inputs[:, i, :, :, :]  # Shape: (batch_size, channels, D, H, W)\n",
    "            print(f\"Shape of scan_input {i}: {scan_input.shape}\")\n",
    "            # Pass the scan through the encoder\n",
    "            latent_rep = encoder(scan_input)  # Get the encoded features\n",
    "            print(f\"Latent representation shape: {latent_rep.shape}\")\n",
    "            \n",
    "            # Flatten the latent representation\n",
    "            latent_rep = latent_rep.view(latent_rep.size(0), -1)  # Flatten to (batch_size, latent_dim)\n",
    "\n",
    "            # Forward pass through the TemporalOrderModel\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(latent_rep)\n",
    "\n",
    "            # Compute the loss for the current scan\n",
    "            loss = criterion(outputs, labels[:, i])\n",
    "            loss.backward()\n",
    "\n",
    "            # Add this loss to the batch loss\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "        # Average loss over the batch\n",
    "        optimizer.step()  # Apply gradients after processing all scans in the batch\n",
    "        total_loss += batch_loss / inputs.size(1)  # Average loss per scan\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Validation model\n",
    "def validate_model(val_loader, model, encoder, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"numpy\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            batch_loss = 0\n",
    "            for i in range(inputs.size(1)):  # Loop over each scan in the batch\n",
    "                scan_input = inputs[:, i, :, :, :]\n",
    "                latent_rep = encoder(scan_input)  # Get the latent features\n",
    "\n",
    "                latent_rep = latent_rep.view(latent_rep.size(0), -1)  # Flatten the latent representation\n",
    "\n",
    "                # Forward pass through TemporalOrderModel\n",
    "                outputs = model(latent_rep)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels[:, i])\n",
    "                batch_loss += loss.item()\n",
    "\n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels[:, i]).sum().item()\n",
    "\n",
    "            total_loss += batch_loss / inputs.size(1)  # Average loss per scan\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalOrderModel architecture: TemporalOrderModel(\n",
      "  (fc1): Linear(in_features=2097152, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n",
      "Autoencoder architecture: AutoencoderKL(\n",
      "  (encoder): Encoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Convolution(\n",
      "        (conv): Conv3d(4, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "      (1-2): 2 x AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (3): AEKLDownsample(\n",
      "        (pad): AsymmetricPad()\n",
      "        (conv): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
      "        )\n",
      "      )\n",
      "      (4): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Convolution(\n",
      "          (conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (5): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (6): AEKLDownsample(\n",
      "        (pad): AsymmetricPad()\n",
      "        (conv): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
      "        )\n",
      "      )\n",
      "      (7): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Convolution(\n",
      "          (conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (8): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (9): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (10): Convolution(\n",
      "        (conv): Conv3d(256, 8, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (blocks): ModuleList(\n",
      "      (0): Convolution(\n",
      "        (conv): Conv3d(8, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "      (1-2): 2 x AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (3): UpSample(\n",
      "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='nearest')\n",
      "        (postconv): Convolution(\n",
      "          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (4): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Convolution(\n",
      "          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (5): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (6): UpSample(\n",
      "        (upsample_non_trainable): Upsample(scale_factor=(2.0, 2.0, 2.0), mode='nearest')\n",
      "        (postconv): Convolution(\n",
      "          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (7): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Convolution(\n",
      "          (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (8): AEKLResBlock(\n",
      "        (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "        (conv1): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "        (conv2): Convolution(\n",
      "          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        )\n",
      "        (nin_shortcut): Identity()\n",
      "      )\n",
      "      (9): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "      (10): Convolution(\n",
      "        (conv): Conv3d(64, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (quant_conv_mu): Convolution(\n",
      "    (conv): Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (quant_conv_log_sigma): Convolution(\n",
      "    (conv): Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      "  (post_quant_conv): Convolution(\n",
      "    (conv): Conv3d(8, 8, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      ")\n",
      "Shape of scan_input 0: torch.Size([4, 176, 256, 240])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected number of channels in input to be divisible by num_groups, but got input of shape [64, 176, 256, 240] and num_groups=32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 61\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_order_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[93], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, encoder, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of scan_input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscan_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Pass the scan through the encoder\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m latent_rep \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscan_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get the encoded features\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatent representation shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_rep\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Flatten the latent representation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/monai/networks/nets/autoencoderkl.py:288\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 288\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/monai/networks/nets/autoencoderkl.py:133\u001b[0m, in \u001b[0;36mAEKLResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    132\u001b[0m     h \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 133\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(h)\n\u001b[1;32m    135\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(h)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py:287\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:2561\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2560\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected number of channels in input to be divisible by num_groups, but got input of shape [64, 176, 256, 240] and num_groups=32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from monai.networks.nets import AutoencoderKL\n",
    "\n",
    "# Define the TemporalOrderModel class\n",
    "class TemporalOrderModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(TemporalOrderModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # Output 3 classes for 3 possible orders\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the encoder and model\n",
    "autoencoder_model = AutoencoderKL(\n",
    "    spatial_dims=3,\n",
    "    in_channels=4,  # Update to 4 channels if your data has 4 channels\n",
    "    out_channels=1,\n",
    "    latent_channels=8,\n",
    "    channels=[64, 128, 256],\n",
    "    num_res_blocks=2,\n",
    "    norm_num_groups=32,\n",
    "    norm_eps=1e-06,\n",
    "    attention_levels=[False, False, False],\n",
    "    with_encoder_nonlocal_attn=False,\n",
    "    with_decoder_nonlocal_attn=False,\n",
    "    include_fc=False\n",
    ")\n",
    "\n",
    "\n",
    "input_size = 8 * 64 * 64 * 64  # Update based on actual latent dimensions (this is an example)\n",
    "temporal_order_model = TemporalOrderModel(input_size=input_size)\n",
    "\n",
    "# Move models to the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "temporal_order_model.to(device)\n",
    "autoencoder_model.to(device)\n",
    "\n",
    "# Define optimizer, using both temporal_order_model and autoencoder_model (if fine-tuning the encoder)\n",
    "optimizer = optim.Adam(\n",
    "    list(temporal_order_model.parameters()) + list(autoencoder_model.parameters()), \n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# Check model architecture for debugging\n",
    "print(\"TemporalOrderModel architecture:\", temporal_order_model)\n",
    "print(\"Autoencoder architecture:\", autoencoder_model)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(train_loader, temporal_order_model, autoencoder_model.encoder, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_accuracy = validate_model(val_loader, temporal_order_model, autoencoder_model.encoder, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
